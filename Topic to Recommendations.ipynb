{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7f84f3",
   "metadata": {
    "papermill": {
     "duration": 0.009414,
     "end_time": "2024-02-12T23:55:46.978004",
     "exception": false,
     "start_time": "2024-02-12T23:55:46.968590",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22369f29",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-12T23:55:47.001395Z",
     "iopub.status.busy": "2024-02-12T23:55:47.000403Z",
     "iopub.status.idle": "2024-02-12T23:55:57.195604Z",
     "shell.execute_reply": "2024-02-12T23:55:57.194269Z"
    },
    "papermill": {
     "duration": 10.211274,
     "end_time": "2024-02-12T23:55:57.198915",
     "exception": false,
     "start_time": "2024-02-12T23:55:46.987641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\verma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\verma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.losses import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578097e",
   "metadata": {
    "papermill": {
     "duration": 0.009542,
     "end_time": "2024-02-12T23:55:57.218415",
     "exception": false,
     "start_time": "2024-02-12T23:55:57.208873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0071c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('preprocessed_data_embeddings.pkl'):\n",
    "    # Load preprocessed data\n",
    "    with open('preprocessed_data_embeddings.pkl', 'rb') as f:\n",
    "        preprocessed_data = pickle.load(f)\n",
    "    df,tfidf_matrix,tfidf_vectorizer = preprocessed_data['df'], preprocessed_data['tfidf_matrix'], preprocessed_data['tfidf_vectorizer']\n",
    "else:\n",
    "    FILE = 'arxiv-metadata-oai-snapshot.json'\n",
    "    def get_data():\n",
    "        with open(FILE) as f:\n",
    "            for line in f:\n",
    "                yield line\n",
    "\n",
    "\n",
    "    dataframe = {\n",
    "        \"submitter\": [],\n",
    "        \"authors\": [],\n",
    "        \"title\": [],\n",
    "        \"doi\": [],\n",
    "        \"categories\": [],\n",
    "        \"abstract\": [],\n",
    "        \"update_date\": []\n",
    "    }\n",
    "\n",
    "    data = get_data()\n",
    "    for i, paper in enumerate(data):\n",
    "        paper = json.loads(paper)\n",
    "        try:\n",
    "            date = int(paper['update_date'].split('-')[0])\n",
    "            if date > 2019:\n",
    "                dataframe['submitter'].append(paper['submitter'])\n",
    "                dataframe['authors'].append(paper['authors'])\n",
    "                dataframe['title'].append(paper['title'])\n",
    "                dataframe['doi'].append(paper['doi'])\n",
    "                dataframe['categories'].append(paper['categories'])\n",
    "                dataframe['abstract'].append(paper['abstract'])\n",
    "                dataframe['update_date'].append(paper['update_date'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    df = pd.DataFrame(dataframe)\n",
    "    del dataframe\n",
    "    df['length'] = df['abstract'].str.len()\n",
    "    def word_count(x):\n",
    "        return len(x.split())\n",
    "\n",
    "    df['word_count'] = df['abstract'].apply(word_count)\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['abstract'])\n",
    "    preprocessed_data = {\n",
    "        'df': df, 'tfidf_vectorizer':tfidf_vectorizer, 'tfidf_matrix': tfidf_matrix}\n",
    "    with open('preprocessed_data_embeddings.pkl', 'wb') as f:\n",
    "        pickle.dump(preprocessed_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e45cd515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Recommendations for watermarking:\n",
      "New Evaluation Metrics Capture Quality Degradation due to LLM\n",
      "  Watermarking\n",
      "\n",
      "A Survey of Text Watermarking in the Era of Large Language Models\n",
      "\n",
      "ItoV: Efficiently Adapting Deep Learning-based Image Watermarking to\n",
      "  Video Watermarking\n",
      "\n",
      "Knowledge-Free Black-Box Watermark and Ownership Proof for Image\n",
      "  Classification Neural Networks\n",
      "\n",
      "Cryptographic switching functions for multiplicative watermarking in\n",
      "  cyber-physical systems\n",
      "\n",
      "Reversible Watermarking in Deep Convolutional Neural Networks for\n",
      "  Integrity Authentication\n",
      "\n",
      "A survey of deep neural network watermarking techniques\n",
      "\n",
      "Publicly Detectable Watermarking for Language Models\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ask user for topic of interest\n",
    "topic_of_interest = input(\"Enter your topic of interest: \")\n",
    "resultsno = int(input(\"Enter the number of results you want: \"))\n",
    "\n",
    "# Preprocess user input\n",
    "# (You can use your existing word_tokenize function)\n",
    "processed_topic = word_tokenize(topic_of_interest.lower())\n",
    "\n",
    "# Convert list of processed words back to string\n",
    "processed_topic_str = \" \".join(processed_topic)\n",
    "user_tfidf = tfidf_vectorizer.transform([processed_topic_str])\n",
    "cosine_similarities = cosine_similarity(user_tfidf, tfidf_matrix).flatten()\n",
    "\n",
    "# Get indices of top recommendations\n",
    "top_indices = cosine_similarities.argsort()[-resultsno:][::-1]\n",
    "\n",
    "# Display top recommendations\n",
    "print(f\"Top Recommendations for {topic_of_interest}:\")\n",
    "j=0\n",
    "for idx in top_indices:\n",
    "    print(j,df['title'][idx])\n",
    "    print()\n",
    "    j+=1\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 612177,
     "sourceId": 7603313,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30474,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 752.633834,
   "end_time": "2024-02-13T00:08:06.365030",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-12T23:55:33.731196",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
